{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynDiffix Usage Tutorial\n",
    "\n",
    "This notebook demonstrates how to use __SynDiffix__, an open-source library for generating statistically-accurate\n",
    "and strongly anonymous synthetic data from structured data.\n",
    "\n",
    "We'll go through the process of loading and inspecting a dataset, creating synthetic datasets that mimics the original, and computing some statistical properties over the two datasets and comparing them.\n",
    "\n",
    "### Setup\n",
    "\n",
    "The `syndiffix` package requires Python 3.10 or later. Let's install it and other packages we'll need for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:54:49.057143Z",
     "start_time": "2024-11-01T06:54:46.347210Z"
    }
   },
   "source": [
    "%pip install -q syndiffix requests pandas scipy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "We'll use the `loan` dataset from the Czech banking dataset. A cleaned-up version is available at open-diffix.org."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:54:53.726862Z",
     "start_time": "2024-11-01T06:54:49.058145Z"
    }
   },
   "source": [
    "import requests\n",
    "import bz2\n",
    "import pickle\n",
    "def download_and_load(url):\n",
    "    response = requests.get(url)\n",
    "    data = bz2.decompress(response.content)\n",
    "    df = pickle.loads(data)\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df_loan = download_and_load('http://open-diffix.org/datasets/loan.pbz2')\n",
    "print(df_loan.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loan_id account_id  loan_date  amount  duration  payments status  defaulted\n",
      "0    5314       1787 1993-07-05   96396        12    8033.0      B       True\n",
      "1    5316       1801 1993-07-11  165960        36    4610.0      A      False\n",
      "2    6863       9188 1993-07-28  127080        60    2118.0      A      False\n",
      "3    5325       1843 1993-08-03  105804        36    2939.0      A      False\n",
      "4    7240      11013 1993-09-06  274740        60    4579.0      A      False\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating synthetic datasets\n",
    "\n",
    "Before creating synthetic datasets, it may be necessary to identify if there is some entity in the data whose privacy must be protected. We call this the *protected entity*. The `loans` dataset has an `account_id` column. Since the account is related to individual persons, we want to ensure that the privacy of individual accounts are protected.\n",
    "\n",
    "To do this, we prepare a dataframe consisting of only the `account_id`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:54:53.730473Z",
     "start_time": "2024-11-01T06:54:53.726862Z"
    }
   },
   "source": [
    "df_pid = df_loan[['account_id']]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the correlation between the `amount` attribute and the `duration` and `loan_id` (we expect strong correlation with `duration` and none with `loan_id`). To do this, we'll create two synthetic datasets of two columns each."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:54:55.952808Z",
     "start_time": "2024-11-01T06:54:53.730473Z"
    }
   },
   "source": [
    "from syndiffix import Synthesizer\n",
    "\n",
    "df_amt_dur = Synthesizer(df_loan[['amount','duration']], pids=df_pid).sample()\n",
    "df_amt_lid = Synthesizer(df_loan[['amount','loan_id']], pids=df_pid).sample()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the Spearman rank-order correlation to measure the correlation, and compare the results for both the original and synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:54:55.965107Z",
     "start_time": "2024-11-01T06:54:55.953809Z"
    }
   },
   "source": [
    "import scipy.stats\n",
    "\n",
    "print(\"amount <-> duration:\")\n",
    "print(\"Original\",scipy.stats.spearmanr(df_loan['amount'], df_loan['duration']))\n",
    "print(\"Synthetic\",scipy.stats.spearmanr(df_amt_dur['amount'], df_amt_dur['duration']))\n",
    "print(\"amount <-> loan_id:\")\n",
    "print(\"Original\",scipy.stats.spearmanr(df_loan['amount'], df_loan['loan_id']))\n",
    "print(\"Synthetic\",scipy.stats.spearmanr(df_amt_lid['amount'], df_amt_lid['loan_id']))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount <-> duration:\n",
      "Original SignificanceResult(statistic=0.6276759903171304, pvalue=5.408495176711555e-76)\n",
      "Synthetic SignificanceResult(statistic=0.6511223290686926, pvalue=1.6781349781407404e-83)\n",
      "amount <-> loan_id:\n",
      "Original SignificanceResult(statistic=-0.037362151151157305, pvalue=0.32992360906471985)\n",
      "Synthetic SignificanceResult(statistic=-0.07869088506745305, pvalue=0.03993317227520811)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations computed from the synthetic data are very close to those of the original data.  As expected, we see a strong correlation between loan amount and loan duration, and virtually no correlation between loan amount and the loan id.\n",
    "\n",
    "### A simpler (but less accurate) approach\n",
    "\n",
    "Having to create a separate synthetic dataset for each column pair is inconvenient. It would be easier to create one synthetic data containing all of the columns. This is how other synthetic data products work. Let's try that and look at the resulting correlations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:55:00.382155Z",
     "start_time": "2024-11-01T06:54:55.965107Z"
    }
   },
   "source": [
    "df_loan_syn = Synthesizer(df_loan, pids=df_pid).sample()\n",
    "\n",
    "print(\"amount <-> duration:\")\n",
    "print(\"Original\",scipy.stats.spearmanr(df_loan['amount'], df_loan['duration']))\n",
    "print(\"Synthetic (2-col)\",scipy.stats.spearmanr(df_amt_dur['amount'], df_amt_dur['duration']))\n",
    "print(\"Synthetic (all)\",scipy.stats.spearmanr(df_loan_syn['amount'], df_loan_syn['duration']))\n",
    "print(\"amount <-> loan_id:\")\n",
    "print(\"Original\",scipy.stats.spearmanr(df_loan['amount'], df_loan['loan_id']))\n",
    "print(\"Synthetic (2-col)\",scipy.stats.spearmanr(df_amt_lid['amount'], df_amt_lid['loan_id']))\n",
    "print(\"Synthetic (all)\",scipy.stats.spearmanr(df_loan_syn['amount'], df_loan_syn['loan_id']))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount <-> duration:\n",
      "Original SignificanceResult(statistic=0.6276759903171304, pvalue=5.408495176711555e-76)\n",
      "Synthetic (2-col) SignificanceResult(statistic=0.6511223290686926, pvalue=1.6781349781407404e-83)\n",
      "Synthetic (all) SignificanceResult(statistic=0.6516572880013952, pvalue=1.4657826844543665e-83)\n",
      "amount <-> loan_id:\n",
      "Original SignificanceResult(statistic=-0.037362151151157305, pvalue=0.32992360906471985)\n",
      "Synthetic (2-col) SignificanceResult(statistic=-0.07869088506745305, pvalue=0.03993317227520811)\n",
      "Synthetic (all) SignificanceResult(statistic=-0.10224091551966587, pvalue=0.007581165934274805)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Spearman measures are only slightly less accurate when all columns are synthesized. This is the case here because there are relatively few columns in this dataset. As a rule, the more columns, the lower the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "Now we give an example of using **SynDiffix** to build an ML model. Here we want to build a model that predicts the `duration` of a loan. \n",
    "\n",
    "Here are the possible values:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:55:00.387146Z",
     "start_time": "2024-11-01T06:55:00.383238Z"
    }
   },
   "source": [
    "print(\"Load Durations (months):\", df_loan['duration'].unique())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Durations (months): [12 36 60 24 48]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the `sklearn` DecisionTreeClassifier. Let's prepare the dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:55:01.127182Z",
     "start_time": "2024-11-01T06:55:00.387724Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Let's drop 'loan_id' because we know it is of no predictive value\n",
    "df = df_loan.drop(columns=[\"loan_id\"])\n",
    "# Change date to a float because DecisionTreeClassifier requires it\n",
    "df['date'] = df['date'].astype('int64') / 10**9\n",
    "# Make the PID dataframe (we did this before, but put it here again for completeness)\n",
    "df_pid = df[['account_id']]\n",
    "# Drop the PID from the dataset because it also has no predictive value\n",
    "df = df.drop(columns=[\"account_id\"])"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\syndiffix-cZ-2Fms3-py3.12\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3789\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3790\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3791\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m df \u001B[38;5;241m=\u001B[39m df_loan\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloan_id\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Change date to a float because DecisionTreeClassifier requires it\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mint64\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m10\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m9\u001B[39m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Make the PID dataframe (we did this before, but put it here again for completeness)\u001B[39;00m\n\u001B[0;32m     11\u001B[0m df_pid \u001B[38;5;241m=\u001B[39m df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccount_id\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\syndiffix-cZ-2Fms3-py3.12\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3891\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3892\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3893\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3895\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\syndiffix-cZ-2Fms3-py3.12\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3793\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3794\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3795\u001B[0m     ):\n\u001B[0;32m   3796\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3797\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3798\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3799\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3800\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3801\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'date'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the synthetic data. Setting `target_column` improves the quality of the synthetic data with respect to building an ML model for the target. If we were to build another ML model for another target, we'd make a new synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "target = 'duration'\n",
    "df_syn = Synthesizer(df, pids=df_pid, target_column=target).sample()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to build models from both the original and synthetic data so that we can compare the results. We need to run all of the modeling preperation steps on both the original and synthetic data.\n",
    "\n",
    "Note that there is a one-hot encoding step here (`pd.get_dummies()`). It is import to synthesize the data **before** one-hot encoding rather than after, especially if there are a lot of values to be encoded. This is because the quality of the synthetic data decreases with an increase in the number of columns.\n",
    "\n",
    "Note also that in order to test the quality of the synthetic data model, we must take the test data from the original data, not from the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the data into features (X) and the target variable (y)\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "X_syn = df_syn.drop(target, axis=1)\n",
    "y_syn = df_syn[target]\n",
    "\n",
    "# And we need to convert strings to one-hot encoding\n",
    "# (Important to do this after synthesis, not before)\n",
    "X = pd.get_dummies(X)\n",
    "X_syn = pd.get_dummies(X_syn)\n",
    "\n",
    "# Split the original dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Do the same for the synthetic data, but noting that we'll use the original test set for testing both original and synthetic\n",
    "X_train_syn, _, y_train_syn, _ = train_test_split(X_syn, y_syn, test_size=0.3, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an run the models, and display prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def runModel(X_train, X_test, y_train, y_test, dataSource):\n",
    "    # Create a decision tree classifier and fit it to the training data\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained classifier to make predictions on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy {dataSource} is {accuracy}\")\n",
    "\n",
    "runModel(X_train, X_test, y_train, y_test, \"Original\")\n",
    "runModel(X_train_syn, X_test, y_train_syn, y_test, \"Synthetic Target\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the synthetic data model is almost 10% below that of the original data model. This is not horrible, but certainly we'd like to do better. We have some ideas in mind as to how to improve this, but may not get to them for a while. If you are interested in contributing, do let us know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
