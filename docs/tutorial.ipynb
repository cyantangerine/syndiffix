{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynDiffix Usage Tutorial\n",
    "\n",
    "This notebook demonstrates how to use __SynDiffix__, an open-source library for generating statistically-accurate\n",
    "and strongly anonymous synthetic data from structured data.\n",
    "\n",
    "We'll go through the process of loading and inspecting a toy dataset, creating a synthetic dataset that mimics the original,\n",
    "computing some statistical properties over the two datasets and comparing them, training ML models to predict a target column\n",
    "from a set of feature columns, and, finally, seeing how to improve accuracy when working synthetic data.\n",
    "\n",
    "### Setup\n",
    "\n",
    "The `syndiffix` package requires Python 3.10 or later. We can install it using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q syndiffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a toy dataset to play with and a way to train and evaluate ML models, so let's install the `scikit-learn` package in order to use one of their popular reference datasets and ML API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to compute some statistical properties of the original and synthetic datasets in order to compare them, so let's install the `scipy` package as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data\n",
    "\n",
    "For this tutorial, we are going to use the Diabetes dataset, which is a popular reference dataset containing some attributes of patients with diabetes and the progression of their illness one year after baseline.\n",
    "\n",
    "You can find more info about it [here](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset).\n",
    "You can see all the available toy datasets [here](https://scikit-learn.org/stable/datasets/toy_dataset.html).\n",
    "\n",
    "First, let's load our data and display some summary information about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, total serum cholesterol\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, total cholesterol / HDL\n",
      "      - s5      ltg, possibly log of serum triglycerides level\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 442 entries, 0 to 441\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     442 non-null    float64\n",
      " 1   sex     442 non-null    float64\n",
      " 2   bmi     442 non-null    float64\n",
      " 3   bp      442 non-null    float64\n",
      " 4   s1      442 non-null    float64\n",
      " 5   s2      442 non-null    float64\n",
      " 6   s3      442 non-null    float64\n",
      " 7   s4      442 non-null    float64\n",
      " 8   s5      442 non-null    float64\n",
      " 9   s6      442 non-null    float64\n",
      " 10  target  442 non-null    float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 38.1 KB\n",
      "None\n",
      "          age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2    0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
      "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
      "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
      "439  0.041708  0.050680 -0.015906  0.017293 -0.037344 -0.013840 -0.024993   \n",
      "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
      "441 -0.045472 -0.044642 -0.073030 -0.081413  0.083740  0.027809  0.173816   \n",
      "\n",
      "           s4        s5        s6  target  \n",
      "0   -0.002592  0.019907 -0.017646   151.0  \n",
      "1   -0.039493 -0.068332 -0.092204    75.0  \n",
      "2   -0.002592  0.002861 -0.025930   141.0  \n",
      "3    0.034309  0.022688 -0.009362   206.0  \n",
      "4   -0.002592 -0.031988 -0.046641   135.0  \n",
      "..        ...       ...       ...     ...  \n",
      "437 -0.002592  0.031193  0.007207   178.0  \n",
      "438  0.034309 -0.018114  0.044485   104.0  \n",
      "439 -0.011080 -0.046883  0.015491   132.0  \n",
      "440  0.026560  0.044529 -0.025930   220.0  \n",
      "441 -0.039493 -0.004222  0.003064    57.0  \n",
      "\n",
      "[442 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "data = sklearn.datasets.load_diabetes(as_frame=True)\n",
    "print(data.DESCR)\n",
    "print(data.frame.info())\n",
    "print(data.frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the attribute correlations in this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SignificanceResult(statistic=0.19782187832853038, pvalue=2.806132121751573e-05)\n",
      "SignificanceResult(statistic=0.03740081502886254, pvalue=0.4328318674689041)\n",
      "SignificanceResult(statistic=0.5613820101065616, pvalue=4.567023927725032e-38)\n",
      "SignificanceResult(statistic=0.4162408981534322, pvalue=5.992783653793038e-20)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "print(scipy.stats.spearmanr(data.frame['target'], data.frame['age']))\n",
    "print(scipy.stats.spearmanr(data.frame['target'], data.frame['sex']))\n",
    "print(scipy.stats.spearmanr(data.frame['target'], data.frame['bmi']))\n",
    "print(scipy.stats.spearmanr(data.frame['target'], data.frame['bp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a moderate correlation between disease progression and body mass index / blood pressure and a low or no correlation with age and sex.\n",
    "\n",
    "### Creating a synthetic dataset\n",
    "\n",
    "Data with health information about individuals is usually privacy-sensitive and can't be shared freely with non-authorized analysts.\n",
    "Fortunately, using __SynDiffix__ we can create a synthetic dataset that preserves most of the statistical properties of the data while, at the same time, protecting subjects' privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 443 entries, 0 to 442\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     443 non-null    float64\n",
      " 1   sex     443 non-null    float64\n",
      " 2   bmi     443 non-null    float64\n",
      " 3   bp      443 non-null    float64\n",
      " 4   s1      443 non-null    float64\n",
      " 5   s2      443 non-null    float64\n",
      " 6   s3      443 non-null    float64\n",
      " 7   s4      443 non-null    float64\n",
      " 8   s5      443 non-null    float64\n",
      " 9   s6      443 non-null    float64\n",
      " 10  target  443 non-null    float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 38.2 KB\n",
      "None\n",
      "          age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0   -0.088905 -0.044642 -0.037305 -0.040099 -0.067338 -0.088821  0.052322   \n",
      "1    0.009016 -0.044642 -0.000500 -0.040099 -0.074853 -0.085983  0.026550   \n",
      "2   -0.079125  0.050680 -0.008266 -0.005670 -0.073144 -0.090673 -0.032356   \n",
      "3    0.001751 -0.044642 -0.043524  0.063187 -0.071959 -0.122398  0.011824   \n",
      "4    0.001751 -0.044642 -0.038737 -0.026328 -0.110069 -0.109479  0.037595   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "438  0.016281  0.050680  0.001849  0.063187  0.119611 -0.015136 -0.043401   \n",
      "439 -0.081394  0.050680  0.011128  0.059809  0.156819  0.214441 -0.099395   \n",
      "440  0.005383  0.050680  0.061575  0.056301  0.073067 -0.111693 -0.039719   \n",
      "441  0.005383  0.050680  0.043952  0.056301  0.165972  0.149143 -0.081969   \n",
      "442  0.067136  0.050680  0.006728 -0.026328  0.181453  0.228561 -0.124072   \n",
      "\n",
      "           s4        s5        s6      target  \n",
      "0   -0.117146 -0.023874 -0.030072   76.491255  \n",
      "1   -0.076395 -0.058212  0.019633   73.999352  \n",
      "2   -0.081556 -0.026251 -0.011759   79.227868  \n",
      "3   -0.076395 -0.046175 -0.013504   64.753065  \n",
      "4   -0.076395 -0.068346 -0.013504   76.586393  \n",
      "..        ...       ...       ...         ...  \n",
      "438  0.071210  0.083759  0.015491  232.524734  \n",
      "439  0.071210  0.033654 -0.035545  199.510470  \n",
      "440  0.085809  0.188901  0.019633  128.000000  \n",
      "441  0.071210  0.037915 -0.059114  275.633304  \n",
      "442  0.119353  0.054718 -0.009085  243.951183  \n",
      "\n",
      "[443 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "from syndiffix import Synthesizer\n",
    "\n",
    "syn_data = Synthesizer(data.frame).sample()\n",
    "\n",
    "print(syn_data.info())\n",
    "print(syn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's measure the same correlations over the synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SignificanceResult(statistic=0.17345404400718656, pvalue=0.0002441557504350876)\n",
      "SignificanceResult(statistic=0.2411171890218794, pvalue=2.797989956139425e-07)\n",
      "SignificanceResult(statistic=0.5808023410914687, pvalue=2.5668562405391346e-41)\n",
      "SignificanceResult(statistic=0.17674262251174974, pvalue=0.00018477785741403383)\n"
     ]
    }
   ],
   "source": [
    "print(scipy.stats.spearmanr(syn_data['target'], syn_data['age']))\n",
    "print(scipy.stats.spearmanr(syn_data['target'], syn_data['sex']))\n",
    "print(scipy.stats.spearmanr(syn_data['target'], syn_data['bmi']))\n",
    "print(scipy.stats.spearmanr(syn_data['target'], syn_data['bp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between the `target` and the `bmi` attributes is preserved well, but the others are distorted.\n",
    "This happens because noise is produced during anonymization and synthesization.\n",
    "The greater the number of columns in the input and the fewer the rows, the noisier the output gets.\n",
    "\n",
    "### Improving accuracy\n",
    "\n",
    "We can make our analysis more accurate by not synthesizing unnecessary columns.\n",
    "When computing correlations, we only need 2 attributes at each step, so we can create a custom synthetic dataset for each computation separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SignificanceResult(statistic=0.21779152058432844, pvalue=3.8110517111859475e-06)\n",
      "SignificanceResult(statistic=0.022247251382116088, pvalue=0.6405103897214186)\n",
      "SignificanceResult(statistic=0.525951951432131, pvalue=7.007965289921266e-33)\n",
      "SignificanceResult(statistic=0.4285132239138389, pvalue=3.627752739586672e-21)\n"
     ]
    }
   ],
   "source": [
    "syn_data = Synthesizer(data.frame[['target', 'age']]).sample()\n",
    "print(scipy.stats.spearmanr(syn_data['target'], syn_data['age']))\n",
    "\n",
    "syn_data = Synthesizer(data.frame[['target', 'sex']]).sample()\n",
    "print(scipy.stats.spearmanr(syn_data['target'], syn_data['sex']))\n",
    "\n",
    "syn_data = Synthesizer(data.frame[['target', 'bmi']]).sample()\n",
    "print(scipy.stats.spearmanr(syn_data['target'], syn_data['bmi']))\n",
    "\n",
    "syn_data = Synthesizer(data.frame[['target', 'bp']]).sample()\n",
    "print(scipy.stats.spearmanr(syn_data['target'], syn_data['bp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the computed correlations are now close to the originals, making the utility of such an analysis high.\n",
    "\n",
    "### Training a ML model with synthetic data\n",
    "\n",
    "In this section, we are going to use a simple linear regression model to predict the `target` column given the other attributes.\n",
    "We are going to create the model twice, once on the raw data and again on the synthetic data, and see how the predictive power\n",
    "of the model varies between the two approaches.\n",
    "\n",
    "First, we need to split the raw data into a training dataset and a test dataset. Furthermore, we'll separate the attribute columns from the target column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_test = data.frame.iloc[:80, :]\n",
    "raw_data_test_y = raw_data_test[\"target\"]\n",
    "raw_data_test_X = raw_data_test.drop(columns=[\"target\"])\n",
    "\n",
    "raw_data_train = data.frame.iloc[80:, :]\n",
    "raw_data_train_y = raw_data_train[\"target\"]\n",
    "raw_data_train_X = raw_data_train.drop(columns=[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll fit a model on the raw training dataset and use it to predict the `target` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40482699584417636"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.linear_model, sklearn.metrics\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression().fit(raw_data_train_X, raw_data_train_y)\n",
    "sklearn.metrics.r2_score(raw_data_test_y, model.predict(raw_data_test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a synthetic dataset from the raw training data and then repeat the previous process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21163884597541138"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_data_train = Synthesizer(raw_data_train).sample()\n",
    "syn_data_train_y = syn_data_train[\"target\"]\n",
    "syn_data_train_X = syn_data_train.drop(columns=[\"target\"])\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression().fit(syn_data_train_X, syn_data_train_y)\n",
    "sklearn.metrics.r2_score(raw_data_test_y, model.predict(raw_data_test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the predictive power of the model is significantly lower.\n",
    "Fortunately, we can pass a focus column to the `Synthesizer` which will create a synthetic dataset better suited for ML tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29617271972853276"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_data_train = Synthesizer(raw_data_train, target_column=\"target\").sample()\n",
    "syn_data_train_y = syn_data_train[\"target\"]\n",
    "syn_data_train_X = syn_data_train.drop(columns=[\"target\"])\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression().fit(syn_data_train_X, syn_data_train_y)\n",
    "sklearn.metrics.r2_score(raw_data_test_y, model.predict(raw_data_test_X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
